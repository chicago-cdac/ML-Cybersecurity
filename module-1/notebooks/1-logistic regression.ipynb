{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A practical guide to constructing a classifier (in Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutoral, you will learn to construct a linear classification model for spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "Loading and processing dataset (cf module 1.1, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We start importing some modules and running some magic commands\n",
    "%matplotlib inline\n",
    "\n",
    "# General math and plotting modules.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Machine Learning library. \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "    \n",
    "X, y = make_classification(n_features=2, \n",
    "                           n_redundant=0, \n",
    "                           n_informative=2,\n",
    "                           random_state=1, \n",
    "                           n_clusters_per_class=1)\n",
    "\n",
    "rng = np.random.RandomState(2)\n",
    "X += 3*rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key ingredient\n",
    "\n",
    "Constructing loss function for binary classification(cf module 1.1, 1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    return the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1/ (1 + np.exp(-z))\n",
    "def lossFunction(theta, X, y ,Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array of theta, X, and y to return the regularize loss function and gradient\n",
    "    of a logistic regression\n",
    "    \"\"\"\n",
    "    m=len(y)\n",
    "    y=y[:,np.newaxis]\n",
    "    predictions = sigmoid(X @ theta)\n",
    "    error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions))\n",
    "    loss = 1/m * sum(error)\n",
    "    regLoss = loss + Lambda/(2*m) * sum(theta**2)\n",
    "    \n",
    "    # compute gradient\n",
    "    j_0= 1/m * (X.transpose() @ (predictions - y))[0]\n",
    "    j_1 = 1/m * (X.transpose() @ (predictions - y))[1:] + (Lambda/m)* theta[1:]\n",
    "    grad= np.vstack((j_0[:,np.newaxis],j_1))\n",
    "    return regLoss[0], grad\n",
    "\n",
    "# Initialize fitting parameters\n",
    "initial_theta = np.zeros((X.shape[1], 1))\n",
    "# Set regularization parameter lambda to 1\n",
    "Lambda = 0.1\n",
    "Eta = 0.01\n",
    "Tolerance=1e-6\n",
    "\n",
    "#Compute and display initial loss and gradient for regularized logistic regression\n",
    "loss, grad=lossFunction(initial_theta, X, y, Lambda)\n",
    "print(\"Loss at initial theta (zeros):\",loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your classifier\n",
    "\n",
    "There are many algorithmic tools for training your classifier. Here, we use the popular gradient descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,eta,Lambda,tolerance):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of eta\n",
    "    \n",
    "    return theta and the list of the loss of theta during each iteration\n",
    "    \"\"\"\n",
    "    m=len(y)\n",
    "    Loss_history =[]\n",
    "    \n",
    "    prev_loss = 10000\n",
    "    loss, grad = lossFunction(theta,X,y,Lambda)\n",
    "\n",
    "    while abs(loss - prev_loss) > tolerance:\n",
    "        theta = theta - (eta * grad)\n",
    "        prev_loss = loss\n",
    "        loss, grad = lossFunction(theta,X,y,Lambda)\n",
    "        \n",
    "        Loss_history.append(loss)\n",
    "    \n",
    "    return theta , Loss_history\n",
    "\n",
    "theta, Loss_history = gradientDescent(X,y,initial_theta,Eta,Lambda,Tolerance)\n",
    "print(\"The regularized theta using ridge regression:\\n\",theta)\n",
    "\n",
    "plt.plot(Loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$Loss(\\Theta)$\")\n",
    "plt.title(\"Loss function using Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test module\n",
    "\n",
    "We still need a method to evaluate the model constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifierPredict(theta,X):\n",
    "    \"\"\"\n",
    "    take in numpy array of theta and X and predict the class \n",
    "    \"\"\"\n",
    "    predictions = X.dot(theta)\n",
    "    \n",
    "    return predictions>0\n",
    "\n",
    "p=classifierPredict(theta,X_train)\n",
    "print(\"Train Accuracy:\", \n",
    "      (sum(p==y_train[:,np.newaxis])/len(y_train) *100)[0],\"%\")\n",
    "\n",
    "p=classifierPredict(theta,X_test)\n",
    "print(\"Test Accuracy:\", \n",
    "      (sum(p==y_test[:,np.newaxis])/len(y_test) *100)[0],\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run\n",
    "\n",
    "Putting everything together, let's take a look at the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# plot the dataset\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "            cmap=cm_bright,edgecolors='k')\n",
    "# and testing points\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test,\n",
    "            cmap=cm_bright, alpha=0.6,edgecolors='k')\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = sigmoid(grid @ theta).reshape(xx.shape)\n",
    "\n",
    "plt.contour(xx,yy,np.round(probs),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise, you will write you own loss function for a spam classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
